<Title>GPT: Ground Zeroes of the AI Revolution</Title>
<Tagline>Exploring the foundations of modern language models</Tagline>

In late 2019, OpenAI released GPT-2, which was the start of...well...all of this. It uses the transformer architecture which was developed by Google.

GPT looks something like this:

![GPT Architecture](/images/GPT1.png "Generative Pre-trained Transformer Architecture")

I'll go over all the different components in the [Components Tab](https://guide.repleteai.com/Text-Generation/Components)