<Title>Generative Pretrained Transformers</Title>
<Tagline>Ground Zeroes of the 21st Century AI Boom</Tagline>

In late 2019, OpenAI released GPT-2, which was the start of...well...all of this. It uses the transformer architecture which was developed by Google.

GPT looks something like this:

![GPT Architecture](/images/gpt1.webp "Generative Pre-trained Transformer Architecture")

There is a lot to say about transformers, and to avoid confusing anyone, we've split up the modules into their own tabs in the Components folder.