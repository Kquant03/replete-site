<Title>Phi</Title>
<Tagline>Textbooks Are All You Need</Tagline>

Phi-2, Phi-3 and Phi-4 are smaller variants of language models which have been created by Microsoft. They have been trained on a smaller amount of filtered web data and synthetic data.

Somehow, with half the data with half the parameter count, Phi models seem to punch way higher than their weight class. The theory behind this is that data quality is much, much more important than the quantity of data you gather. This is highlighted in the publications: [Textbooks Are All You Need](https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need/) and [Textbooks Are All You Need II](https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need-ii-phi-1-5-technical-report/)

After some research, the only difference from Llama to Phi/Phi-1.5/Phi-2 are the PhiDecoderLayer, where they use PhiAttention, and PhiMLP layers.

Phi-3, on the other hand...has the "Phi3SuScaledRotaryEmbedding" and "Phi3YarnScaledRotaryEmbedding", where they are used to extend the context of the rotary embeddings...while the query, key and values are fused, and the MLPâ€™s up and gate projection layers are also fused.

